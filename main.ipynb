{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 1115394\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = Path('data') / 'tinyshakespeare' / 'input.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"{len(text) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(chars) = 65\n",
      "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(f\"{len(chars) = }\")\n",
    "print(repr(''.join(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: [itos[i] for i in l]\n",
    "''.join(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = int(len(data) * .9)\n",
    "train_data = data[:idx]\n",
    "val_data   = data[idx:]\n",
    "\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context -> target\n",
      "tensor([18]) -> tensor(47)\n",
      "tensor([18, 47]) -> tensor(56)\n",
      "tensor([18, 47, 56]) -> tensor(57)\n",
      "tensor([18, 47, 56, 57]) -> tensor(58)\n",
      "tensor([18, 47, 56, 57, 58]) -> tensor(1)\n",
      "tensor([18, 47, 56, 57, 58,  1]) -> tensor(15)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) -> tensor(47)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> tensor(58)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "print('context -> target')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(context, '->', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingCharacterDataset(Dataset):\n",
    "    def __init__(self, data: torch.tensor, block_size=8):\n",
    "        assert data.dim() == 1\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.data[idx:idx+self.block_size],\n",
    "            self.data[idx+1:idx+self.block_size+1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1, 47, 52, 55, 59, 47, 56, 43],\n",
       "         [52, 42,  1, 40, 43, 63, 53, 52],\n",
       "         [41, 50, 53, 59, 42, 57,  1, 39],\n",
       "         [45,  1, 39, 52, 42,  1, 44, 53]]),\n",
       " tensor([[47, 52, 55, 59, 47, 56, 43, 11],\n",
       "         [42,  1, 40, 43, 63, 53, 52, 42],\n",
       "         [50, 53, 59, 42, 57,  1, 39, 56],\n",
       "         [ 1, 39, 52, 42,  1, 44, 53, 56]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SlidingCharacterDataset(data=train_data, block_size=8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "x, y = next(iter(train_dataloader))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape     = torch.Size([4, 8])\n",
      "y.shape     = torch.Size([4, 8])\n",
      "y_hat.shape = torch.Size([4, 8, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nggEU:KWq;Sr!thQ:3n-ayxeAz$xcTJhCwsw3X\\n.jGeVnffL-zANYwRG&$ep?Jy mp.kyqIWd'T;n-w\\nMhAAQDYprbR3Su3ZNVVX\",\n",
       " \"\\nsP hyb3WKh!FdyiWN?$I$NqI.FXKxxjC3lb;soba'SfuG&l,hwtf cWHl!PXadeSJ?Jy,a!zrs'aSG3xmakKkBcZZUixQN?u3aDi\",\n",
       " \"\\nwtzsyavgI:eQ'$pDv-cW mmbWaQRrHy\\n3X3kduWNFAY-.AB\\nMd-ZaZcdN-WciDH.&Aabw-$W:KyLg$pNW'vG3z$lbwq3pgNB:\\nIx\",\n",
       " \"\\nZoXdt 'qI-wn-pgsIG'HF'zsYZLsFqI-Z---C &tDW&-dn--u33wbmAVY-z&?dBkQVKaDnVfh.uho;ZET'yn-OLPy&'iQPB:j,gs\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first index of a bigram is used as a context and the second is used as a target\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size = len(chars)):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x  = self.token_embedding_table(x)\n",
    "        return x\n",
    "\n",
    "def generate(\n",
    "        model: nn.Module,\n",
    "        start_token: torch.tensor = torch.zeros(4, 1, dtype=torch.long), # B, T\n",
    "        max_iter: int = 100\n",
    "        ):\n",
    "    sequence = start_token\n",
    "    for _ in range(max_iter):\n",
    "        logits = model(sequence[:,-1])\n",
    "        proba = F.softmax(logits, dim=1)\n",
    "        pick = torch.multinomial(proba, num_samples=1)\n",
    "        sequence = torch.cat([sequence, pick], dim=1)\n",
    "    return sequence\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "y_hat = model(x)\n",
    "print(f\"{x.shape     = }\")\n",
    "print(f\"{y.shape     = }\")\n",
    "print(f\"{y_hat.shape = }\")\n",
    "\n",
    "sequence = generate(model)\n",
    "[''.join(decode(l)) for l in sequence.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7419, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_hat.view(-1, len(chars)), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   1/5][train loss]    2.45248\n",
      "[epoch   2/5][train loss]    2.45254\n",
      "[epoch   3/5][train loss]    2.45256\n",
      "[epoch   4/5][train loss]    2.45257\n",
      "[epoch   5/5][train loss]    2.45256\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "epoch = 5\n",
    "model.train()\n",
    "for e in range(epoch):\n",
    "    running_loss = 0\n",
    "    for i, (x, y) in enumerate(DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True), start=1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat.view(-1, len(chars)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print(f\"[epoch {e+1:>3}/{epoch}][train loss] {running_loss/i:>10.5f}\", end='\\r')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Betig ape g dle.\n",
      "N:\n",
      "Indeyofo andifoull n\n",
      "Wivopand.\n",
      "ULofango y gaviser fa'd hir I.\n",
      "\n",
      "As chind pu f the ind be, pugind, we\n",
      "ICHES:\n",
      "Th fo:\n",
      "Thal mind luck ono lls t ire; tire,\n",
      "ENGothant,\n",
      "Whton werod han y h\n"
     ]
    }
   ],
   "source": [
    "sequence = generate(model, start_token=torch.zeros(1, 1, dtype=torch.long), max_iter=200)\n",
    "print(''.join(decode(sequence[0].tolist())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
