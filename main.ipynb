{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 1115394\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path('data') / 'tinyshakespeare' / 'input.txt'\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"{len(text) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(chars) = 65\n",
      "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(f\"{len(chars) = }\")\n",
    "print(repr(''.join(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: [itos[i] for i in l]\n",
    "''.join(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = int(len(data) * .9)\n",
    "train_data = data[:idx]\n",
    "val_data   = data[idx:]\n",
    "\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context -> target\n",
      "tensor([18]) -> tensor(47)\n",
      "tensor([18, 47]) -> tensor(56)\n",
      "tensor([18, 47, 56]) -> tensor(57)\n",
      "tensor([18, 47, 56, 57]) -> tensor(58)\n",
      "tensor([18, 47, 56, 57, 58]) -> tensor(1)\n",
      "tensor([18, 47, 56, 57, 58,  1]) -> tensor(15)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) -> tensor(47)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> tensor(58)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "print('context -> target')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(context, '->', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingCharacterDataset(Dataset):\n",
    "    def __init__(self, data: torch.tensor, block_size=8):\n",
    "        assert data.dim() == 1\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.data[idx:idx+self.block_size],\n",
    "            self.data[idx+1:idx+self.block_size+1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[49, 43, 50, 63,  1,  5, 58, 47],\n",
       "         [58,  0, 33, 52, 51, 43, 56, 47],\n",
       "         [51,  6,  1, 58, 53,  1, 51, 39],\n",
       "         [ 1, 58, 46, 47, 57,  1, 39, 58]]),\n",
       " tensor([[43, 50, 63,  1,  5, 58, 47, 57],\n",
       "         [ 0, 33, 52, 51, 43, 56, 47, 58],\n",
       "         [ 6,  1, 58, 53,  1, 51, 39, 49],\n",
       "         [58, 46, 47, 57,  1, 39, 58, 58]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SlidingCharacterDataset(data=train_data, block_size=8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "x, y = next(iter(train_dataloader))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape     = torch.Size([4, 8])\n",
      "y.shape     = torch.Size([4, 8])\n",
      "y_hat.shape = torch.Size([4, 8, 65])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"\\noA!ptlZ:Fa&S\\nxoKNY jVHkSpOCsjEonGIXnb$OXYCYwnV'QAKTXC&&GDMRaTwkaPUj$rOK\\nO P o\\nIlCM.ZjjuDL.IJ\\nfqwbAxV\",\n",
       " \"\\n':.OfCg'E,dfK!;$\\n\\nIfpcjKZrYEAo;XKh,bfi;SpTrrNyVlk\\nnWUUZGMGRtlEAAYkRxd,rOTwAl\\n3gVUC.'OXCglm!3tNU,Hptv\",\n",
       " \"\\n\\nU!tykWg.k !AwZcj:FH;oUU?XSj&VTXjVR\\nkck;'vXPf\\nNb\\nsKO cYkIqZqpAmxLyU3XhJd\\n&$HE$Jcr vs,3?yUGKyK,EJaLaC\",\n",
       " \"\\n'KLCg$O'TpT:3?IZGAQv-'gaWAQyG'hghsvOoZYR:FJsrPc&j.PT.CiFcA\\nN$oZASfCLtrViFLTP3PYErstrdKRu\\nN!CgzVCvuyb\"]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first index of a bigram is used as a context and the second is used as a target\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size = len(chars)):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x  = self.token_embedding_table(x)\n",
    "        return x\n",
    "\n",
    "def generate(\n",
    "        model: nn.Module,\n",
    "        start_token: torch.tensor = torch.zeros(4, 1, dtype=torch.long), # B, T\n",
    "        max_iter: int = 100\n",
    "        ):\n",
    "    sequence = start_token\n",
    "    for _ in range(max_iter):\n",
    "        logits = model(sequence[:,-1])\n",
    "        proba = F.softmax(logits, dim=1)\n",
    "        pick = torch.multinomial(proba, num_samples=1)\n",
    "        sequence = torch.cat([sequence, pick], dim=1)\n",
    "    return sequence\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "y_hat = model(x)\n",
    "print(f\"{x.shape     = }\")\n",
    "print(f\"{y.shape     = }\")\n",
    "print(f\"{y_hat.shape = }\")\n",
    "\n",
    "sequence = generate(model)\n",
    "[''.join(decode(l)) for l in sequence.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7857, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(y_hat.view(-1, len(chars)), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
